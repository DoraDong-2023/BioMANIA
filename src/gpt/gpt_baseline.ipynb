{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84252b5b-f228-45a1-8335-962270be9d9e",
   "metadata": {},
   "source": [
    "# Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d9109-7d2e-487e-8b10-2bf3ba99d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### remember modify the LIB before start running\n",
    "LIB = \"ehrapy\"\n",
    "LIB_ALIAS = \"ehrapy\"\n",
    "#### Also notice that the prompt also contains `scanpy` which needs to be modified manually\n",
    "\n",
    "mode_index = 'randomseed' # 'similarseed' or randomseed\n",
    "# whether use similar shot example retriving mode, the similar shot example retriving is \n",
    "# to retrieve the similar queries that similar to the input query\n",
    "# noted that this mode always retrieve 5 shot queries for the same API, as the query for same API is always similar\n",
    "oracle_index = 'noncorrected' # 'noncorrected' or corrected\n",
    "# whether use the corrected mode after retrieving API. The retrieved API is different from retrieved query, \n",
    "# retrieved API will provide an API list for gpt to select\n",
    "# if under correct mode, we will put the ground truth API into the retrieved API list, \n",
    "# and delete the last one API under the retrieved API list\n",
    "retrieved_index = 'retrieved' # nonretrieved or retrieved\n",
    "# if using retrieved mode, then we will provide a filtered retrieved list\n",
    "# otherwise, we will provide the whole API list for gpt to select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_interface, random, bz2, json, re, os\n",
    "from tqdm import auto as tqdm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)  # turn off logging\n",
    "mode = 'openai'\n",
    "k_shot = 5\n",
    "secrets = gpt_interface.setup_openai('../configs/secrets.json', mode=mode)\n",
    "# load data\n",
    "with open(f'../data/standard_process/{LIB}/API_inquiry_annotate.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# load val ids\n",
    "with open(f\"../data/standard_process/{LIB}/API_instruction_testval_query_ids.json\", 'r') as file:\n",
    "    files_ids = json.load(file)\n",
    "\n",
    "test = [dict(query=row['query'], gold=row['api_name']) for row in [i for i in data if i['query_id'] in files_ids['test']]]\n",
    "val = [dict(query=row['query'], gold=row['api_name']) for row in [i for i in data if i['query_id'] in files_ids['val']]]\n",
    "train_remain = [dict(query=row['query'], gold=row['api_name']) for row in [i for i in data if (i['query_id'] not in files_ids['val']) and (i['query_id'] not in files_ids['test'])]]\n",
    "\n",
    "print('train:', len(train_remain), 'val:', len(val), 'test:', len(test))\n",
    "\n",
    "# add K-shot \n",
    "shuffled = [dict(query=row['query'], gold=row['api_name']) for row in [i for i in data if i['query_id'] not in files_ids['val'] and i['query_id'] not in files_ids['test']]]\n",
    "random.Random(0).shuffle(shuffled)\n",
    "print(len(shuffled))\n",
    "print(len(data))\n",
    "assert len(data)==len(test)+len(val)+len(shuffled)\n",
    "train = shuffled[:k_shot]\n",
    "# all-apis\n",
    "from utils import get_all_api_json\n",
    "all_apis, all_apis_json = get_all_api_json(f\"../data/standard_process/{LIB}/API_init.json\", mode='full')\n",
    "len(all_apis), len(all_apis_json)\n",
    "# load API_init\n",
    "with open(f'../data/standard_process/{LIB}/API_init.json', 'r') as f:\n",
    "    API_init = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "class ToolRetriever:\n",
    "    def __init__(self, corpus_tsv_path = \"\", model_path=\"\",shuffled_data=[]):\n",
    "        self.build_retrieval_corpus(corpus_tsv_path, model_path,shuffled_data)\n",
    "    def build_retrieval_corpus(self, corpus_tsv_path, model_path,shuffled_data):\n",
    "        print(\"Building corpus...\")\n",
    "        self.corpus_tsv_path = corpus_tsv_path\n",
    "        self.model_path = model_path\n",
    "        documents_df = pd.read_csv(self.corpus_tsv_path, sep='\\t')\n",
    "        corpus, self.corpus2tool = process_retrieval_document_query_version(documents_df)\n",
    "        corpus_ids = list(corpus.keys())\n",
    "        corpus = [corpus[cid] for cid in corpus_ids]\n",
    "        self.corpus = corpus\n",
    "        self.embedder = SentenceTransformer(self.model_path, device=device)\n",
    "        self.corpus_embeddings = self.embedder.encode(self.corpus, convert_to_tensor=True)\n",
    "        self.shuffled_data = shuffled_data\n",
    "        self.shuffled_queries = [item['query'] for item in shuffled_data]\n",
    "        self.shuffled_query_embeddings = self.embedder.encode(self.shuffled_queries, convert_to_tensor=True)\n",
    "    def retrieving(self, query, top_k):\n",
    "        query_embedding = self.embedder.encode(query, convert_to_tensor=True)\n",
    "        hits = util.semantic_search(query_embedding, self.corpus_embeddings, top_k=top_k, score_function=util.cos_sim) #170*\n",
    "        retrieved_apis = [self.corpus2tool[self.corpus[hit['corpus_id']]] for hit in hits[0]]\n",
    "        #scores = [hit['score'] for hit in hits[0]]\n",
    "        return retrieved_apis[:top_k]\n",
    "    def retrieve_similar_queries(self, query, shot_k=5):\n",
    "        query_embedding = self.embedder.encode(query, convert_to_tensor=True)\n",
    "        # filter class/composite API\n",
    "        hits = util.semantic_search(query_embedding, self.shuffled_query_embeddings, top_k=shot_k+10, score_function=util.cos_sim)\n",
    "        hits = [hit for hit in hits if self.shuffled_data[hit['corpus_id']]['gold'].startswith(LIB_ALIAS)]\n",
    "        hits = [hit for hit in hits if API_init[self.shuffled_data[hit['corpus_id']]['gold']]['api_type'] not in ['class', 'unknown']]\n",
    "        hits = hits[:shot_k]\n",
    "        #similar_queries = [shuffled_data[hit['corpus_id']] for hit in hits[0]]\n",
    "        similar_queries = [\"\\nInstruction: \" + self.shuffled_data[hit['corpus_id']]['query'] + \"\\nFunction: [\" + self.shuffled_data[hit['corpus_id']]['gold']+\"]\" for hit in hits[0]]\n",
    "        return ''.join(similar_queries)\n",
    "\n",
    "def process_retrieval_document_query_version(documents_df):\n",
    "    ir_corpus = {}\n",
    "    corpus2tool = {}\n",
    "    for row in documents_df.itertuples():\n",
    "        doc = json.loads(row.document_content)\n",
    "        ir_corpus[row.docid] = compress_api_str_from_list_query_version(doc)\n",
    "        corpus2tool[compress_api_str_from_list_query_version(doc)] = doc['api_calling'][0].split('(')[0]\n",
    "    return ir_corpus, corpus2tool\n",
    "\n",
    "def compress_api_str_from_list_query_version(api):\n",
    "    api_name = api['api_calling'][0].split('(')[0]\n",
    "    api_desc_truncated = api['api_description'].split('\\n')[0]\n",
    "    req_params = json.dumps(api['required_parameters'])\n",
    "    opt_params = json.dumps(api['optional_parameters'])\n",
    "    return_schema = json.dumps(api['Returns'])\n",
    "    compressed_str = f\"{api_name}, {api_desc_truncated}, required_params: {req_params}, optional_params: {opt_params}, return_schema: {return_schema}\"\n",
    "    return compressed_str\n",
    "\n",
    "import hashlib, random\n",
    "from utils import correct_pred, get_sampled_shuffled, generate_seed\n",
    "\n",
    "import pandas as pd\n",
    "device = 'cuda:0'\n",
    "retriever = ToolRetriever(corpus_tsv_path=f\"../data/standard_process/{LIB}/retriever_train_data/corpus.tsv\", model_path=f\"../hugging_models/retriever_model_finetuned/{LIB}/assigned/\",shuffled_data=shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf93a2-898b-4db0-b036-68d9707207ab",
   "metadata": {},
   "source": [
    "# Query to API selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76cea28-ee96-4a90-9985-5c176572c54d",
   "metadata": {},
   "source": [
    "## K-shot\n",
    "\n",
    "Here, GPT does not see candidate list of APIs, it just tries to tell the correct function from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59abc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_generate_prompt\n",
    "prompt = get_generate_prompt()\n",
    "print(prompt)\n",
    "\n",
    "def run_gpt(test, gpt_model, prompt, dout, mode, max_tokens=20,title=\"\"):\n",
    "    correct = []\n",
    "    for ex in (pbar := tqdm.tqdm(test)):\n",
    "        if (not ex['gold'].startswith(LIB_ALIAS)) or (ex['gold'] not in API_init):# do not test composite API\n",
    "            print('filters composite API')\n",
    "            continue\n",
    "        elif (API_init[ex['gold']]['api_type'] in ['class', 'unknown']): # do not test class type API query\n",
    "            print('filters class API')\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        if mode_index == 'similarseed':\n",
    "            similar_queries = retriever.retrieve_similar_queries(ex['query'],shot_k=5)\n",
    "        elif mode_index == 'randomseed':\n",
    "            #sampled_shuffled = random.sample(shuffled, 5)\n",
    "            shot_k=5\n",
    "            sampled_shuffled = get_sampled_shuffled(ex['gold'], shuffled, num_samples=shot_k+10)\n",
    "            sampled_shuffled = [hit for hit in sampled_shuffled if hit['gold'].startswith(LIB_ALIAS)]\n",
    "            sampled_shuffled = [hit for hit in sampled_shuffled if API_init[hit['gold']]['api_type'] not in ['class', 'unknown']]\n",
    "            sampled_shuffled = sampled_shuffled[:shot_k]\n",
    "            assert len(sampled_shuffled)==shot_k\n",
    "            similar_queries = \"\".join([\"\\nInstruction: \" + iii['query'] + \"\\nFunction: [\" + iii['gold']+\"]\" for iii in sampled_shuffled])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # do not provide retrieved part\n",
    "        p = gpt_interface.query_openai(prompt.format(lib_name=LIB_ALIAS, query=ex['query'],similar_queries=similar_queries), mode=mode, model=gpt_model, max_tokens=max_tokens)\n",
    "        p = p.replace('[','').replace(']','')\n",
    "        parts = p.split(',')\n",
    "        result = []\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            part = correct_pred(part, LIB_ALIAS)\n",
    "            result.append(part)\n",
    "        p = result\n",
    "        print('==>ask: ')\n",
    "        print(prompt.format(lib_name=LIB_ALIAS, query=ex['query'],similar_queries=similar_queries))\n",
    "        print(f'==>answer: {p}')\n",
    "        ex['pred'] = p\n",
    "        ex['correct'] = c = ex['pred'][0] == ex['gold']\n",
    "        ex['prompt'] = prompt.format(lib_name=LIB_ALIAS, query=ex['query'],similar_queries=similar_queries)\n",
    "        correct.append(c)\n",
    "        pbar.set_description('correct: {}'.format(sum(correct)/len(correct)))\n",
    "    with open(os.path.join(dout, '{}.json'.format(title)), 'wt') as f:\n",
    "        title = os.path.join(dout, '{}.json'.format(title))\n",
    "        print(f'save to {title}')\n",
    "        json.dump(test, f, indent=2)\n",
    "\n",
    "import os\n",
    "folder_name = \"{}/{}-shot-generate\".format(LIB,k_shot)\n",
    "print(f'makedir for {folder_name}')\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'gpt-3.5-turbo-0125-trainsample'\n",
    "run_gpt(val, 'gpt-3.5-turbo-0125', prompt, folder_name, mode,title=title)\n",
    "title = f'gpt-3.5-turbo-0125-test'\n",
    "run_gpt(test, 'gpt-3.5-turbo-0125', prompt, folder_name, mode,title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5bf4c-c263-4ed8-9176-ca553e287a54",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Here, GPT sees the list of available APIs and tries to pick out the correct one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt_new(test, gpt_model, prompt, dout, mode, max_tokens=20,top_k=3,title=\"\"):\n",
    "    correct = []\n",
    "    for ex in (pbar := tqdm.tqdm(test)):\n",
    "        # ignore compositeAPI and classAPI query\n",
    "        if (not ex['gold'].startswith(LIB_ALIAS)) or (ex['gold'] not in API_init):# do not test composite API\n",
    "            print('filters composite API')\n",
    "            continue\n",
    "        elif (API_init[ex['gold']]['api_type'] in ['class', 'unknown']): # do not test class type API query\n",
    "            print('filters class API')\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        if mode_index == 'similarseed':\n",
    "            similar_queries = retriever.retrieve_similar_queries(ex['query'],shot_k=5)\n",
    "        elif mode_index == 'randomseed':\n",
    "            shot_k=5\n",
    "            #sampled_shuffled = random.sample(shuffled, shot_k+10)\n",
    "            sampled_shuffled = get_sampled_shuffled(ex['gold'], shuffled, num_samples=shot_k+10)\n",
    "            # filter composite/class API\n",
    "            #sampled_shuffled = util.semantic_search(query_embedding, self.shuffled_query_embeddings, top_k=shot_k+10, score_function=util.cos_sim)\n",
    "            sampled_shuffled = [hit for hit in sampled_shuffled if hit['gold'].startswith(LIB_ALIAS)]\n",
    "            sampled_shuffled = [hit for hit in sampled_shuffled if API_init[hit['gold']]['api_type'] not in ['class', 'unknown']]\n",
    "            sampled_shuffled = sampled_shuffled[:shot_k]\n",
    "            similar_queries = \"\"\n",
    "            for iii in sampled_shuffled:\n",
    "                tmp_retrieved_api_list = retriever.retrieving(iii['query'], top_k=top_k+10)\n",
    "                tmp_retrieved_api_list = [i for i in tmp_retrieved_api_list if (i.startswith(LIB_ALIAS)) and (i in API_init) and (API_init[i]['api_type'] not in ['class', 'unknown'])]\n",
    "                # for the k-shot incontext example, require the groundtruth API must be in the sub retrieved_api_list, otherwise GPT won't understand the tasks\n",
    "                # noted that this is not a cheating step, as we just did it for incontext examples.\n",
    "                tmp_retrieved_api_list = tmp_retrieved_api_list[:top_k]\n",
    "                if iii['gold'] not in tmp_retrieved_api_list:\n",
    "                    tmp_retrieved_api_list = tmp_retrieved_api_list[:top_k-1]+[iii['gold']]\n",
    "                random.shuffle(tmp_retrieved_api_list)\n",
    "                function_candidates = \"\"\n",
    "                for idx, api in enumerate(tmp_retrieved_api_list):\n",
    "                    if idx<top_k:\n",
    "                        function_candidates += f\"{idx}:\" + api + \", description: \"+all_apis_json[api].replace('\\n',' ')+\"\\n\"\n",
    "                similar_queries += \"function candidates:\\n\" + function_candidates+\"Instruction: \" + iii['query'] + \"\\nFunction: [\" + iii['gold']+\"]\"+ \"\\n---\\n\"\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if retrieved_index=='retrieved':\n",
    "            #retrieved_api_list = retriever.retrieving(ex['query'], top_k=top_k)\n",
    "            retrieved_api_list = retriever.retrieving(ex['query'], top_k=top_k+10)\n",
    "            retrieved_api_list = [i for i in retrieved_api_list if (i.startswith(LIB_ALIAS)) and (i in API_init) and (API_init[i]['api_type'] not in ['class', 'unknown'])]\n",
    "            retrieved_api_list = retrieved_api_list[:top_k]\n",
    "            assert len(retrieved_api_list)==top_k\n",
    "            assert all(i.startswith(LIB_ALIAS) and (API_init[i]['api_type'] not in ['class', 'unknown']) and (i in API_init) for i in retrieved_api_list)\n",
    "            if oracle_index=='corrected':\n",
    "                if ex['gold'] not in retrieved_api_list:\n",
    "                    retrieved_api_list = [ex['gold']] + retrieved_api_list[:-1]\n",
    "                assert ex['gold'] in retrieved_api_list\n",
    "            elif oracle_index=='noncorrected':\n",
    "                pass\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            retrieved_apis = \"\"\n",
    "            for idx, api in enumerate(retrieved_api_list):\n",
    "                retrieved_apis+=f\"{idx}:\" + api+\", description: \"+all_apis_json[api].replace('\\n',' ')+\"\\n\"\n",
    "        elif retrieved_index=='nonretrieved':\n",
    "            retrieved_apis = \"\"\n",
    "            for idx, api in enumerate(all_apis_json):\n",
    "                if (not api.startswith(LIB_ALIAS)) or (API_init[api]['api_type'] in ['class', 'unknown']):\n",
    "                    continue\n",
    "                retrieved_apis+=f\"{idx}:\" + api+\", description: \"+all_apis_json[api].replace('\\n',' ')+\"\\n\"\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        #print(ex['gold'],retrieved_api_list)\n",
    "        if ex['gold'] in retrieved_api_list:# retriever correct\n",
    "            p = gpt_interface.query_openai(prompt.format(query=ex['query'],retrieved_apis=retrieved_apis,similar_queries=similar_queries), mode=mode, model=gpt_model, max_tokens=max_tokens)\n",
    "            p = p.replace('[','').replace(']','')\n",
    "            parts = p.split(',')\n",
    "            result = []\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                part = correct_pred(part, LIB)\n",
    "                result.append(part)\n",
    "            p = result\n",
    "            print('==>ask: ')\n",
    "            print(prompt.format(query=ex['query'],retrieved_apis=retrieved_apis,similar_queries=similar_queries))\n",
    "            print(f'==>answer: {p}')\n",
    "            ex['pred'] = p\n",
    "            ex['correct'] = c = ex['gold']==ex['pred'][0]\n",
    "        else:# retriever wrong\n",
    "            ex['pred'] = None\n",
    "            ex['correct'] = c = False\n",
    "        ex['retrieved_apis'] = retrieved_api_list\n",
    "        ex['prompt'] = prompt.format(query=ex['query'],retrieved_apis=retrieved_apis,similar_queries=similar_queries)\n",
    "        correct.append(c)\n",
    "        pbar.set_description('correct: {}'.format(sum(correct)/len(correct)))\n",
    "    with open(os.path.join(dout, '{}.json'.format(title)), 'wt') as f:\n",
    "        json.dump(test, f, indent=2)\n",
    "\n",
    "import os\n",
    "folder_name = \"{}/{}-shot-classify\".format(LIB,k_shot)\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77666260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_retrieved_prompt, get_nonretrieved_prompt\n",
    "\n",
    "if retrieved_index == 'retrieved':\n",
    "    print('get_retrieved_prompt')\n",
    "    prompt = get_retrieved_prompt()\n",
    "    top_k = 3\n",
    "    title = f'gpt-3.5-turbo-0125-topk-{top_k}-trainsample'\n",
    "    run_gpt_new(val, 'gpt-3.5-turbo-0125', prompt, folder_name, mode,top_k=top_k,title=title)\n",
    "    title = f'gpt-3.5-turbo-0125-topk-{top_k}-test'\n",
    "    run_gpt_new(test, 'gpt-3.5-turbo-0125', prompt, folder_name, mode,top_k=top_k,title=title)\n",
    "elif retrieved_index == 'nonretrieved':\n",
    "    print('get_nonretrieved_prompt')\n",
    "    prompt = get_nonretrieved_prompt()\n",
    "    top_k = 3\n",
    "    title = f'gpt-3.5-turbo-0125-topk-{top_k}-{retrieved_index}-trainsample'\n",
    "    run_gpt_new(val, 'gpt-3.5-turbo-0125', prompt, folder_name, mode,top_k=top_k,title=title)\n",
    "    title = f'gpt-3.5-turbo-0125-topk-{top_k}-{retrieved_index}-test'\n",
    "    run_gpt_new(test, 'gpt-3.5-turbo-0125', prompt, folder_name, mode,top_k=top_k,title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "title = f'gpt-4-0125-preview-topk-{top_k}-trainsample'\n",
    "run_gpt_new(val, 'gpt-4-0125-preview', prompt, folder_name, mode,top_k=top_k,title=title)\n",
    "title = f'gpt-4-0125-preview-topk-{top_k}-test'\n",
    "run_gpt_new(test, 'gpt-4-0125-preview', prompt, folder_name, mode, top_k=top_k,title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5f5b5",
   "metadata": {},
   "source": [
    "### ambiguous pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0569d7a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, re, os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import extract_and_print_adjusted, plot_figure, is_pair_in_merged_pairs, find_similar_two_pairs, correct_entries\n",
    "results = []\n",
    "import glob\n",
    "def get_json_from_local(LIB, LIB_ALIAS):\n",
    "    merged_pairs = find_similar_two_pairs(f'../data/standard_process/{LIB}/API_init.json')\n",
    "    results = []\n",
    "    all_apis_from_pairs = set(api for pair in merged_pairs for api in pair)\n",
    "    collect_json = {}\n",
    "    collect_json_all = {}\n",
    "    for fname in glob.glob(\"*/*/*.json\"):\n",
    "        if f'{LIB}/' not in fname: # filter corresponding LIBs\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        with open(fname, 'r') as file:\n",
    "            res = json.load(file)\n",
    "        # filter compositeAPI and classAPI queries\n",
    "        res = [i for i in res if 'correct' in i]\n",
    "        \"\"\"res_tmp = []\n",
    "        for item in res:\n",
    "            if item['pred']:\n",
    "                # correct it\n",
    "                item['pred'][0] = item['pred'][0].replace('scanpy_subset.', 'scanpy.').replace('sc.', 'scanpy.').replace('ep.', 'ehrapy.').replace('sq.', 'squidpy.').replace('snap.', 'snapatac2.')\n",
    "            res_tmp.append(item)\n",
    "        res = res_tmp\"\"\"\n",
    "        res = correct_entries(res, LIB_ALIAS)\n",
    "        #for item in res:\n",
    "        #    if item['pred'][0]:\n",
    "        #        if item['pred'][0].startswith('scanpy_subset.'):\n",
    "        #            print(item['pred'][0])\n",
    "        original_correct = [ex['correct'] for ex in res]\n",
    "        original_c = [i for i in original_correct if i]\n",
    "        original_accuracy = sum(original_correct) / len(original_correct) if res else 0\n",
    "        if 'generate' in fname:\n",
    "            retrieved_in_gold = []\n",
    "            retrieved_in_gold_correct = []\n",
    "            retrieved_in_gold_accuracy = '-'\n",
    "        else:\n",
    "            retrieved_in_gold = [ex for ex in res if (ex['gold'] in ex['retrieved_apis'])]\n",
    "            retrieved_in_gold_correct = [ex['correct'] for ex in retrieved_in_gold]\n",
    "            retrieved_in_gold_accuracy = sum(retrieved_in_gold_correct) / len(retrieved_in_gold_correct) if retrieved_in_gold_correct else 0\n",
    "        collect_json_all[fname] = []\n",
    "        for item in retrieved_in_gold:\n",
    "            collect_json_all[fname].append({'query': item['query'], 'gold':item['gold'], 'pred':item['pred'][0]})\n",
    "        if 'generate' in fname:\n",
    "            retriever_accuracy = '-'\n",
    "        else:\n",
    "            retriever_accuracy = len(retrieved_in_gold)/len(res) if res else 0\n",
    "            assert abs(retriever_accuracy*retrieved_in_gold_accuracy-original_accuracy)<0.01\n",
    "        #filtered_res = [item for item in res if (item['pred'][0] is not None) and (not is_pair_in_merged_pairs(item['gold'], item['pred'][0][0], merged_pairs))]\n",
    "        filtered_res = []\n",
    "        for item in res:\n",
    "            if item['pred'] is None:\n",
    "                assert not item['correct']\n",
    "                filtered_res.append(item)\n",
    "            elif not is_pair_in_merged_pairs(item['gold'], item['pred'][0], merged_pairs):\n",
    "                filtered_res.append(item)\n",
    "        #\n",
    "        incorrect_filtered_res = [item for item in filtered_res if not item['correct']]\n",
    "        #print(len(retrieved_in_gold), len(incorrect_filtered_res))\n",
    "        collect_json[fname] = []\n",
    "        for item in incorrect_filtered_res:\n",
    "            collect_json[fname].append({'query': item['query'], 'gold':item['gold'], 'pred':item['pred']})\n",
    "        #\n",
    "        filtered_correct = [ex['correct'] for ex in filtered_res]\n",
    "        filtered_c = [i for i in filtered_correct if i]\n",
    "        filtered_accuracy = sum(filtered_correct) / len(filtered_res) if filtered_res else 0\n",
    "        parent_dir = os.path.dirname(fname)\n",
    "        match = re.search('-topk-(\\d+)', os.path.basename(fname))\n",
    "        top_k = int(match.group(1)) if match else '-'\n",
    "        pred_count = [len(i['pred']) for i in res if i['pred'] is not None]\n",
    "        #\n",
    "        if os.path.basename(fname).replace('.json', '').startswith('gpt-4'):\n",
    "            model_name = \"gpt-4\"\n",
    "        else:\n",
    "            model_name = \"gpt-3.5\"\n",
    "        if os.path.basename(fname).replace('.json', '').endswith('trainsample'):\n",
    "            test_val = 'synthetic_val'\n",
    "        elif os.path.basename(fname).replace('.json', '').endswith('trainsub2'):\n",
    "            #test_val = 'synthetic_train_sub2'\n",
    "            #continue\n",
    "            pass\n",
    "        elif os.path.basename(fname).replace('.json', '').endswith('new_val'):\n",
    "            test_val = 'new_val'\n",
    "        else:\n",
    "            test_val = 'human annotate'\n",
    "        if 'nonretrieved' in os.path.basename(fname):\n",
    "            retrieval_status = \"nonretrieved\"\n",
    "        else:\n",
    "            retrieval_status = \"retrieved\"\n",
    "        results.append(dict(\n",
    "            #task=parent_dir,\n",
    "            lib = parent_dir.split('/')[0],\n",
    "            model_name=model_name,\n",
    "            accuracy=original_accuracy,\n",
    "            total=len(res),\n",
    "            #retrieved_in_gold_accuracy = retrieved_in_gold_accuracy,\n",
    "            retrieved_in_gold_total = len(retrieved_in_gold),\n",
    "            filtered_accuracy=filtered_accuracy,\n",
    "            filtered_c = len(filtered_c),\n",
    "            filtered_total = len(filtered_res),\n",
    "            #retriever_accuracy=retriever_accuracy,\n",
    "            top_k=top_k,\n",
    "            pred_mean = sum(pred_count)/len(pred_count),\n",
    "            test_val=test_val,\n",
    "            retrieval_status=retrieval_status\n",
    "        ))\n",
    "    results = pd.DataFrame(results)\n",
    "    results = results.sort_values(by=['lib', 'model_name', 'top_k','test_val', 'retrieval_status'])\n",
    "    result_whole = results.copy()\n",
    "    #results\n",
    "    df = results.copy()\n",
    "    df['has_dash'] = df['top_k'] == \"-\"\n",
    "    model_name_order_original = [\"gpt-3.5\", \"gpt-4\"]\n",
    "    df['model_name'] = pd.Categorical(df['model_name'], categories=model_name_order_original, ordered=True)\n",
    "    df_sorted_again = df.sort_values(by=['has_dash', 'model_name', 'retrieval_status', 'test_val'], ascending=[False, True,True, False])\n",
    "    df_sorted_again.drop('has_dash', axis=1, inplace=True)\n",
    "    return df_sorted_again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba56598",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = \"scanpy\"\n",
    "LIB_ALIAS = \"scanpy\"\n",
    "df = get_json_from_local(LIB, LIB_ALIAS)\n",
    "cluster_data = extract_and_print_adjusted(df)\n",
    "plot_figure(cluster_data, LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bcb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = \"squidpy\"\n",
    "LIB_ALIAS = \"squidpy\"\n",
    "df = get_json_from_local(LIB, LIB_ALIAS)\n",
    "cluster_data = extract_and_print_adjusted(df)\n",
    "plot_figure(cluster_data, LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = \"ehrapy\"\n",
    "LIB_ALIAS = \"ehrapy\"\n",
    "df = get_json_from_local(LIB, LIB_ALIAS)\n",
    "cluster_data = extract_and_print_adjusted(df)\n",
    "plot_figure(cluster_data, LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = \"snapatac2\"\n",
    "LIB_ALIAS = \"snapatac2\"\n",
    "df = get_json_from_local(LIB, LIB_ALIAS)\n",
    "cluster_data = extract_and_print_adjusted(df)\n",
    "plot_figure(cluster_data, LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f447191",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = \"scanpy_subset\"\n",
    "LIB_ALIAS = \"scanpy\"\n",
    "df = get_json_from_local(LIB, LIB_ALIAS)\n",
    "cluster_data = extract_and_print_adjusted(df)\n",
    "plot_figure(cluster_data, LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e56e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
